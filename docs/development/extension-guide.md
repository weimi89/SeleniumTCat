# SeleniumTCat æ“´å±•é–‹ç™¼æŒ‡å—

## æ“´å±•é–‹ç™¼æ¦‚è¦½

SeleniumTCat æ¡ç”¨å¯æ“´å±•çš„æ¶æ§‹è¨­è¨ˆï¼Œæ”¯æ´é–‹ç™¼è€…è¼•é¬†æ–°å¢æ–°çš„åŠŸèƒ½å’Œçˆ¬èŸ²æ¨¡çµ„ã€‚æœ¬æŒ‡å—å°‡è©³ç´°èªªæ˜å¦‚ä½•æ“´å±•ç³»çµ±çš„å„å€‹éƒ¨åˆ†ã€‚

```mermaid
graph TD
    subgraph "æ“´å±•é»"
        NewScraper[æ–°å¢ Scraper]
        NewFeature[æ–°å¢åŠŸèƒ½æ¨¡çµ„]
        NewPlatform[æ–°å¢å¹³å°æ”¯æ´]
        NewConfig[æ–°å¢é…ç½®é¸é …]
    end

    subgraph "æ ¸å¿ƒç³»çµ±"
        BaseScraper[BaseScraper]
        MAM[MultiAccountManager]
        BrowserUtils[browser_utils]
        ConfigSystem[é…ç½®ç³»çµ±]
    end

    NewScraper --> BaseScraper
    NewFeature --> MAM
    NewPlatform --> BrowserUtils
    NewConfig --> ConfigSystem
```

## æ–°å¢ Scraper çˆ¬èŸ²

### 1. åŸºæœ¬çµæ§‹

#### 1.1 å»ºç«‹æ–°çš„ Scraper é¡åˆ¥

```python
# src/scrapers/new_scraper.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import time
from datetime import datetime
from pathlib import Path

# å°å…¥å…±ç”¨æ¨¡çµ„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..'))
from src.utils.windows_encoding_utils import safe_print, check_pythonunbuffered
from src.core.base_scraper import BaseScraper
from src.core.multi_account_manager import MultiAccountManager

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
check_pythonunbuffered()

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class NewScraper(BaseScraper):
    """
    æ–°çš„çˆ¬èŸ²é¡åˆ¥ç¯„ä¾‹
    ç¹¼æ‰¿è‡ª BaseScraperï¼Œå¯¦ä½œç‰¹å®šçš„è³‡æ–™æŠ“å–åŠŸèƒ½
    """

    def __init__(self, username, password, headless=False,
                 download_base_dir="downloads", custom_param=None):
        # å‘¼å«çˆ¶é¡åˆ¥å»ºæ§‹å­
        super().__init__(username, password, headless, download_base_dir)

        # æ–° Scraper ç‰¹æœ‰çš„å±¬æ€§
        self.custom_param = custom_param
        self.data_to_extract = []

    def navigate_to_query_page(self):
        """
        å°èˆªåˆ°æŸ¥è©¢é é¢ - å¯¦ä½œçˆ¶é¡åˆ¥çš„æŠ½è±¡æ–¹æ³•

        Returns:
            bool: å°èˆªæˆåŠŸè¿”å› Trueï¼Œå¤±æ•—è¿”å› False
        """
        safe_print("ğŸ§­ å°èˆªåˆ°æ–°åŠŸèƒ½æŸ¥è©¢é é¢...")

        try:
            # å¯¦ä½œç‰¹å®šçš„å°èˆªé‚è¼¯
            target_url = "https://www.example.com/new-feature"
            self.driver.get(target_url)
            time.sleep(3)

            # æª¢æŸ¥æ˜¯å¦æˆåŠŸåˆ°é”ç›®æ¨™é é¢
            if "new-feature" in self.driver.current_url:
                safe_print("âœ… æˆåŠŸå°èˆªåˆ°æŸ¥è©¢é é¢")
                return True
            else:
                safe_print("âŒ å°èˆªå¤±æ•—")
                return False

        except Exception as e:
            safe_print(f"âŒ å°èˆªéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def download_data(self):
        """
        ä¸‹è¼‰è³‡æ–™ - å¯¦ä½œçˆ¶é¡åˆ¥çš„æŠ½è±¡æ–¹æ³•

        Returns:
            list: ä¸‹è¼‰çš„æª”æ¡ˆæ¸…å–®
        """
        safe_print("ğŸ“¥ é–‹å§‹ä¸‹è¼‰æ–°åŠŸèƒ½è³‡æ–™...")

        # è¨­å®šè‡¨æ™‚ä¸‹è¼‰ç›®éŒ„
        self.setup_temp_download_dir()
        downloaded_files = []

        try:
            # å¯¦ä½œå…·é«”çš„ä¸‹è¼‰é‚è¼¯
            download_success = self._execute_download_process()

            if download_success:
                # è™•ç†ä¸‹è¼‰çš„æª”æ¡ˆ
                raw_files = list(self.download_dir.glob("*.xlsx"))
                renamed_files = self._rename_downloaded_files(raw_files)

                # ç§»å‹•åˆ°æœ€çµ‚ç›®éŒ„ä¸¦æ¸…ç†
                final_files = self.move_and_cleanup_files(raw_files, renamed_files)
                downloaded_files.extend(final_files)

            return downloaded_files

        except Exception as e:
            safe_print(f"âŒ ä¸‹è¼‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def _execute_download_process(self):
        """åŸ·è¡Œä¸‹è¼‰æµç¨‹çš„å…§éƒ¨æ–¹æ³•"""
        try:
            # 1. è¨­å®šæœå°‹æ¢ä»¶
            self._set_search_criteria()

            # 2. åŸ·è¡Œæœå°‹
            search_success = self._perform_search()
            if not search_success:
                return False

            # 3. ä¸‹è¼‰çµæœ
            download_success = self._download_results()
            return download_success

        except Exception as e:
            safe_print(f"âŒ åŸ·è¡Œä¸‹è¼‰æµç¨‹å¤±æ•—: {e}")
            return False

    def _set_search_criteria(self):
        """è¨­å®šæœå°‹æ¢ä»¶"""
        safe_print("ğŸ” è¨­å®šæœå°‹æ¢ä»¶...")
        # å¯¦ä½œæœå°‹æ¢ä»¶è¨­å®šé‚è¼¯
        pass

    def _perform_search(self):
        """åŸ·è¡Œæœå°‹"""
        safe_print("ğŸ” åŸ·è¡Œæœå°‹...")
        # å¯¦ä½œæœå°‹é‚è¼¯
        return True

    def _download_results(self):
        """ä¸‹è¼‰æœå°‹çµæœ"""
        safe_print("ğŸ“¥ ä¸‹è¼‰æœå°‹çµæœ...")
        # å¯¦ä½œä¸‹è¼‰é‚è¼¯
        return True

    def _rename_downloaded_files(self, files):
        """é‡æ–°å‘½åä¸‹è¼‰çš„æª”æ¡ˆ"""
        renamed_files = []

        for i, file_path in enumerate(files):
            # ç”Ÿæˆæ–°çš„æª”æ¡ˆå
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            new_name = f"æ–°åŠŸèƒ½è³‡æ–™_{self.username}_{timestamp}_{i+1}.xlsx"
            new_path = file_path.parent / new_name

            # é‡æ–°å‘½å
            file_path.rename(new_path)
            renamed_files.append(new_path)
            safe_print(f"ğŸ“ æª”æ¡ˆå·²é‡æ–°å‘½å: {file_path.name} â†’ {new_name}")

        return renamed_files


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='æ–°åŠŸèƒ½è‡ªå‹•ä¸‹è¼‰å·¥å…·')
    parser.add_argument('--headless', action='store_true', help='ä½¿ç”¨ç„¡é ­æ¨¡å¼')
    parser.add_argument('--custom-param', type=str, help='è‡ªè¨‚åƒæ•¸')

    args = parser.parse_args()

    try:
        print("ğŸš€ æ–°åŠŸèƒ½è‡ªå‹•ä¸‹è¼‰å·¥å…·")

        manager = MultiAccountManager("accounts.json")
        headless_arg = True if '--headless' in sys.argv else None

        manager.run_all_accounts(
            NewScraper,
            headless_override=headless_arg,
            custom_param=args.custom_param
        )

        return 0

    except Exception as e:
        print(f"â›” éŒ¯èª¤: {e}")
        return 1


if __name__ == "__main__":
    main()
```

### 2. é€²éšåŠŸèƒ½å¯¦ä½œ

#### 2.1 æœƒè©±ç®¡ç†å’ŒéŒ¯èª¤æ¢å¾©

```python
def _handle_session_issues(self):
    """è™•ç†æœƒè©±ç›¸é—œå•é¡Œ"""
    # æª¢æŸ¥æœƒè©±è¶…æ™‚
    if self._check_session_timeout():
        safe_print("â° æª¢æ¸¬åˆ°æœƒè©±è¶…æ™‚ï¼Œå˜—è©¦æ¢å¾©...")
        recovery_success = self._handle_session_timeout()
        if not recovery_success:
            safe_print("âŒ æœƒè©±æ¢å¾©å¤±æ•—")
            return False

    # æª¢æŸ¥å…¶ä»–æœƒè©±å•é¡Œ
    return True

def _handle_custom_alerts(self):
    """è™•ç†ç‰¹å®šçš„å½ˆçª—"""
    try:
        alert = self.driver.switch_to.alert
        alert_text = alert.text
        safe_print(f"ğŸ”” æª¢æ¸¬åˆ°å½ˆçª—: {alert_text}")

        # æ ¹æ“šå½ˆçª—å…§å®¹æ±ºå®šè™•ç†æ–¹å¼
        if "ç‰¹å®šé—œéµå­—" in alert_text:
            safe_print("âœ… æ¥å—ç‰¹å®šæç¤º")
            alert.accept()
            return True
        else:
            safe_print("âš ï¸ æœªçŸ¥å½ˆçª—ï¼Œè¬¹æ…è™•ç†")
            alert.dismiss()
            return False

    except Exception:
        return True  # æ²’æœ‰å½ˆçª—
```

#### 2.2 AJAX å’Œå‹•æ…‹å…§å®¹è™•ç†

```python
def _wait_for_ajax_completion(self, timeout=30):
    """ç­‰å¾… AJAX è«‹æ±‚å®Œæˆ"""
    try:
        # ç­‰å¾… jQuery AJAX å®Œæˆ
        WebDriverWait(self.driver, timeout).until(
            lambda driver: driver.execute_script("return jQuery.active == 0")
        )
        safe_print("âœ… AJAX è«‹æ±‚å·²å®Œæˆ")
        return True
    except Exception as e:
        safe_print(f"âš ï¸ AJAX ç­‰å¾…è¶…æ™‚: {e}")
        return False

def _wait_for_element_clickable(self, selector, timeout=10):
    """ç­‰å¾…å…ƒç´ å¯é»æ“Š"""
    try:
        element = WebDriverWait(self.driver, timeout).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
        )
        return element
    except Exception as e:
        safe_print(f"âš ï¸ å…ƒç´ ä¸å¯é»æ“Š: {selector}, {e}")
        return None
```

## æ–°å¢åŠŸèƒ½æ¨¡çµ„

### 1. æ“´å±• MultiAccountManager

#### 1.1 æ–°å¢ç®¡ç†å™¨åŠŸèƒ½

```python
# src/core/enhanced_account_manager.py
from .multi_account_manager import MultiAccountManager
import asyncio
import concurrent.futures


class EnhancedAccountManager(MultiAccountManager):
    """å¢å¼·çš„å¤šå¸³è™Ÿç®¡ç†å™¨"""

    def __init__(self, config_file="accounts.json"):
        super().__init__(config_file)
        self.parallel_limit = 3  # å¹³è¡Œè™•ç†é™åˆ¶

    async def run_accounts_parallel(self, scraper_class, max_workers=None, **kwargs):
        """å¹³è¡Œè™•ç†å¤šå€‹å¸³è™Ÿ"""
        accounts = self.get_enabled_accounts()
        max_workers = max_workers or min(len(accounts), self.parallel_limit)

        safe_print(f"ğŸš€ é–‹å§‹å¹³è¡Œè™•ç† {len(accounts)} å€‹å¸³è™Ÿ (æœ€å¤§ä½µç™¼: {max_workers})")

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å»ºç«‹ä»»å‹™
            future_to_account = {
                executor.submit(self._process_account_sync, scraper_class, account, **kwargs): account
                for account in accounts
            }

            results = []
            for future in concurrent.futures.as_completed(future_to_account):
                account = future_to_account[future]
                try:
                    result = future.result()
                    results.append(result)
                    safe_print(f"âœ… å¸³è™Ÿ {account['username']} è™•ç†å®Œæˆ")
                except Exception as e:
                    safe_print(f"âŒ å¸³è™Ÿ {account['username']} è™•ç†å¤±æ•—: {e}")
                    results.append({
                        "success": False,
                        "username": account['username'],
                        "error": str(e)
                    })

        return results

    def _process_account_sync(self, scraper_class, account, **kwargs):
        """åŒæ­¥è™•ç†å–®ä¸€å¸³è™Ÿï¼ˆä¾›åŸ·è¡Œç·’æ± ä½¿ç”¨ï¼‰"""
        scraper = scraper_class(
            username=account['username'],
            password=account['password'],
            **kwargs
        )
        return scraper.run_full_process()
```

### 2. æ–°å¢å·¥å…·æ¨¡çµ„

#### 2.1 è³‡æ–™è™•ç†å·¥å…·

```python
# src/utils/data_processor.py
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any


class DataProcessor:
    """è³‡æ–™è™•ç†å·¥å…·é¡åˆ¥"""

    @staticmethod
    def merge_excel_files(file_paths: List[Path], output_path: Path) -> bool:
        """åˆä½µå¤šå€‹ Excel æª”æ¡ˆ"""
        try:
            combined_data = []

            for file_path in file_paths:
                df = pd.read_excel(file_path)
                df['ä¾†æºæª”æ¡ˆ'] = file_path.name
                combined_data.append(df)

            # åˆä½µè³‡æ–™
            merged_df = pd.concat(combined_data, ignore_index=True)
            merged_df.to_excel(output_path, index=False)

            safe_print(f"âœ… å·²åˆä½µ {len(file_paths)} å€‹æª”æ¡ˆåˆ° {output_path}")
            return True

        except Exception as e:
            safe_print(f"âŒ åˆä½µæª”æ¡ˆå¤±æ•—: {e}")
            return False

    @staticmethod
    def generate_summary_report(data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        total_accounts = len(data)
        successful_accounts = sum(1 for item in data if item.get('success', False))
        total_downloads = sum(len(item.get('downloads', [])) for item in data)

        return {
            "ç¸½å¸³è™Ÿæ•¸": total_accounts,
            "æˆåŠŸå¸³è™Ÿæ•¸": successful_accounts,
            "æˆåŠŸç‡": f"{(successful_accounts/total_accounts)*100:.1f}%" if total_accounts > 0 else "0%",
            "ç¸½ä¸‹è¼‰æª”æ¡ˆæ•¸": total_downloads,
            "å¹³å‡æ¯å¸³è™Ÿä¸‹è¼‰": f"{total_downloads/successful_accounts:.1f}" if successful_accounts > 0 else "0"
        }
```

## æ–°å¢å¹³å°æ”¯æ´

### 1. æ“´å±•ç€è¦½å™¨æ”¯æ´

#### 1.1 æ–°å¢ Firefox æ”¯æ´

```python
# src/core/firefox_utils.py
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager
from pathlib import Path
import os


def init_firefox_browser(headless=False, download_dir="downloads"):
    """åˆå§‹åŒ– Firefox ç€è¦½å™¨"""
    try:
        firefox_options = webdriver.FirefoxOptions()

        if headless:
            firefox_options.add_argument('--headless')

        # è¨­å®šä¸‹è¼‰ç›®éŒ„
        download_path = Path(download_dir).absolute()
        download_path.mkdir(parents=True, exist_ok=True)

        firefox_options.set_preference("browser.download.folderList", 2)
        firefox_options.set_preference("browser.download.manager.showWhenStarting", False)
        firefox_options.set_preference("browser.download.dir", str(download_path))
        firefox_options.set_preference("browser.helperApps.neverAsk.saveToDisk",
                                     "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

        # è‡ªå‹•ç®¡ç† GeckoDriver
        service = Service(GeckoDriverManager().install())

        driver = webdriver.Firefox(service=service, options=firefox_options)
        wait = WebDriverWait(driver, 10)

        safe_print("âœ… Firefox ç€è¦½å™¨åˆå§‹åŒ–æˆåŠŸ")
        return driver, wait

    except Exception as e:
        safe_print(f"âŒ Firefox ç€è¦½å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        raise
```

#### 1.2 ç€è¦½å™¨é¸æ“‡æ©Ÿåˆ¶

```python
# src/core/browser_factory.py
from .browser_utils import init_chrome_browser
from .firefox_utils import init_firefox_browser


class BrowserFactory:
    """ç€è¦½å™¨å·¥å» é¡åˆ¥"""

    SUPPORTED_BROWSERS = {
        'chrome': init_chrome_browser,
        'firefox': init_firefox_browser,
    }

    @classmethod
    def create_browser(cls, browser_type='chrome', **kwargs):
        """å»ºç«‹æŒ‡å®šé¡å‹çš„ç€è¦½å™¨"""
        if browser_type not in cls.SUPPORTED_BROWSERS:
            raise ValueError(f"ä¸æ”¯æ´çš„ç€è¦½å™¨é¡å‹: {browser_type}")

        init_func = cls.SUPPORTED_BROWSERS[browser_type]
        return init_func(**kwargs)

    @classmethod
    def get_available_browsers(cls):
        """å–å¾—å¯ç”¨çš„ç€è¦½å™¨æ¸…å–®"""
        available = []
        for browser_type in cls.SUPPORTED_BROWSERS:
            try:
                # å˜—è©¦åˆå§‹åŒ–ç€è¦½å™¨
                driver, wait = cls.create_browser(browser_type, headless=True)
                driver.quit()
                available.append(browser_type)
            except Exception:
                pass
        return available
```

## æ–°å¢é…ç½®é¸é …

### 1. æ“´å±•é…ç½®ç³»çµ±

#### 1.1 é€²éšé…ç½®çµæ§‹

```python
# src/core/advanced_config.py
import json
from pathlib import Path
from typing import Dict, Any, Optional


class AdvancedConfig:
    """é€²éšé…ç½®ç®¡ç†å™¨"""

    def __init__(self, config_file="accounts.json", schema_file="config_schema.json"):
        self.config_file = config_file
        self.schema_file = schema_file
        self.config = {}
        self.schema = {}

        self.load_schema()
        self.load_config()

    def load_schema(self):
        """è¼‰å…¥é…ç½®æª”æ¡ˆçµæ§‹å®šç¾©"""
        if Path(self.schema_file).exists():
            with open(self.schema_file, 'r', encoding='utf-8') as f:
                self.schema = json.load(f)

    def load_config(self):
        """è¼‰å…¥ä¸¦é©—è­‰é…ç½®"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        self.validate_config()

    def validate_config(self):
        """é©—è­‰é…ç½®ç¬¦åˆçµæ§‹å®šç¾©"""
        if not self.schema:
            return True

        # å¯¦ä½œé…ç½®é©—è­‰é‚è¼¯
        errors = self._validate_against_schema(self.config, self.schema)
        if errors:
            raise ValueError(f"é…ç½®é©—è­‰å¤±æ•—: {'; '.join(errors)}")

    def _validate_against_schema(self, config, schema):
        """æ ¹æ“šçµæ§‹å®šç¾©é©—è­‰é…ç½®"""
        errors = []
        # å¯¦ä½œè©³ç´°çš„é©—è­‰é‚è¼¯
        return errors

    def get_scraper_config(self, scraper_name: str) -> Dict[str, Any]:
        """å–å¾—ç‰¹å®šçˆ¬èŸ²çš„é…ç½®"""
        scraper_configs = self.config.get('scrapers', {})
        return scraper_configs.get(scraper_name, {})

    def get_global_setting(self, setting_name: str, default=None):
        """å–å¾—å…¨åŸŸè¨­å®š"""
        return self.config.get('settings', {}).get(setting_name, default)
```

#### 1.2 é…ç½®æª”æ¡ˆçµæ§‹ç¯„ä¾‹

```json
{
  "accounts": [
    {
      "username": "account1",
      "password": "password1",
      "enabled": true,
      "profile": "standard"
    }
  ],
  "settings": {
    "headless": false,
    "download_base_dir": "downloads",
    "browser_type": "chrome",
    "parallel_limit": 3,
    "retry_attempts": 3,
    "timeout": {
      "page_load": 30,
      "element_wait": 10,
      "download": 60
    }
  },
  "scrapers": {
    "payment_scraper": {
      "default_period": 1,
      "max_periods": 5,
      "file_prefix": "å®¢æ¨‚å¾—å°å¸³å–®"
    },
    "freight_scraper": {
      "default_date_range": 30,
      "file_prefix": "ç™¼ç¥¨æ˜ç´°"
    }
  },
  "profiles": {
    "standard": {
      "timeout_multiplier": 1.0,
      "retry_attempts": 3
    },
    "slow_connection": {
      "timeout_multiplier": 2.0,
      "retry_attempts": 5
    }
  }
}
```

## æ¸¬è©¦å’Œé©—è­‰

### 1. å–®å…ƒæ¸¬è©¦ç¯„æœ¬

```python
# tests/unit/test_new_scraper.py
import pytest
from unittest.mock import Mock, patch, MagicMock
from src.scrapers.new_scraper import NewScraper


class TestNewScraper:
    """NewScraper å–®å…ƒæ¸¬è©¦"""

    @pytest.fixture
    def scraper(self):
        """å»ºç«‹æ¸¬è©¦ç”¨çš„ NewScraper å¯¦ä¾‹"""
        return NewScraper("test_user", "test_pass", custom_param="test")

    @patch('src.scrapers.new_scraper.init_chrome_browser')
    def test_navigate_to_query_page_success(self, mock_init_browser, scraper):
        """æ¸¬è©¦æˆåŠŸå°èˆªåˆ°æŸ¥è©¢é é¢"""
        # è¨­å®šæ¨¡æ“¬å°è±¡
        mock_driver = Mock()
        mock_driver.current_url = "https://www.example.com/new-feature"
        mock_init_browser.return_value = (mock_driver, Mock())

        scraper.driver = mock_driver

        # åŸ·è¡Œæ¸¬è©¦
        result = scraper.navigate_to_query_page()

        # é©—è­‰çµæœ
        assert result is True
        mock_driver.get.assert_called_once()

    def test_download_data_with_custom_param(self, scraper):
        """æ¸¬è©¦ä½¿ç”¨è‡ªè¨‚åƒæ•¸ä¸‹è¼‰è³‡æ–™"""
        # è¨­å®šæ¨¡æ“¬ç’°å¢ƒ
        scraper.driver = Mock()
        scraper.download_dir = Mock()

        with patch.object(scraper, '_execute_download_process') as mock_download:
            mock_download.return_value = True

            result = scraper.download_data()

            assert isinstance(result, list)
            mock_download.assert_called_once()
```

### 2. æ•´åˆæ¸¬è©¦ç¯„æœ¬

```python
# tests/integration/test_new_scraper_integration.py
import pytest
from src.scrapers.new_scraper import NewScraper
from src.core.multi_account_manager import MultiAccountManager


class TestNewScraperIntegration:
    """NewScraper æ•´åˆæ¸¬è©¦"""

    @pytest.fixture
    def test_config(self, tmp_path):
        """å»ºç«‹æ¸¬è©¦ç”¨é…ç½®æª”æ¡ˆ"""
        config_file = tmp_path / "test_accounts.json"
        config_data = {
            "accounts": [
                {
                    "username": "test_user",
                    "password": "test_pass",
                    "enabled": True
                }
            ],
            "settings": {
                "headless": True,
                "download_base_dir": str(tmp_path / "downloads")
            }
        }

        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f)

        return str(config_file)

    def test_full_workflow_with_manager(self, test_config):
        """æ¸¬è©¦èˆ‡ MultiAccountManager çš„å®Œæ•´å·¥ä½œæµç¨‹"""
        manager = MultiAccountManager(test_config)

        # é€™è£¡å¯èƒ½éœ€è¦æ¨¡æ“¬ç¶²ç«™å›æ‡‰
        with patch('src.scrapers.new_scraper.NewScraper.navigate_to_query_page') as mock_nav:
            mock_nav.return_value = True

            results = manager.run_all_accounts(NewScraper, custom_param="integration_test")

            assert len(results) == 1
            assert results[0]['username'] == 'test_user'
```

## éƒ¨ç½²å’Œæ‰“åŒ…

### 1. å»ºç«‹å®‰è£è…³æœ¬

```python
# scripts/install_extension.py
#!/usr/bin/env python3
"""æ“´å±•æ¨¡çµ„å®‰è£è…³æœ¬"""

import shutil
from pathlib import Path


def install_new_scraper():
    """å®‰è£æ–°çš„çˆ¬èŸ²æ¨¡çµ„"""
    source_dir = Path("extensions/new_scraper")
    target_dir = Path("src/scrapers")

    if not source_dir.exists():
        print("âŒ æ‰¾ä¸åˆ°æ“´å±•æ¨¡çµ„ç›®éŒ„")
        return False

    # è¤‡è£½æª”æ¡ˆ
    for file_path in source_dir.glob("*.py"):
        target_path = target_dir / file_path.name
        shutil.copy2(file_path, target_path)
        print(f"âœ… å·²å®‰è£: {file_path.name}")

    print("ğŸ‰ æ“´å±•æ¨¡çµ„å®‰è£å®Œæˆ")
    return True


if __name__ == "__main__":
    install_new_scraper()
```

### 2. ç‰ˆæœ¬ç®¡ç†

```python
# src/core/extension_manager.py
class ExtensionManager:
    """æ“´å±•ç®¡ç†å™¨"""

    def __init__(self):
        self.extensions = {}
        self.load_extensions()

    def load_extensions(self):
        """è¼‰å…¥æ‰€æœ‰å¯ç”¨çš„æ“´å±•"""
        extension_dir = Path("src/scrapers")

        for file_path in extension_dir.glob("*_scraper.py"):
            module_name = file_path.stem
            try:
                # å‹•æ…‹è¼‰å…¥æ¨¡çµ„
                spec = importlib.util.spec_from_file_location(module_name, file_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # å°‹æ‰¾ Scraper é¡åˆ¥
                scraper_class = getattr(module, f"{module_name.title().replace('_', '')}")
                self.extensions[module_name] = scraper_class

            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ“´å±• {module_name} å¤±æ•—: {e}")

    def get_available_scrapers(self):
        """å–å¾—å¯ç”¨çš„çˆ¬èŸ²æ¸…å–®"""
        return list(self.extensions.keys())

    def create_scraper(self, scraper_name, **kwargs):
        """å»ºç«‹æŒ‡å®šçš„çˆ¬èŸ²å¯¦ä¾‹"""
        if scraper_name not in self.extensions:
            raise ValueError(f"æœªçŸ¥çš„çˆ¬èŸ²é¡å‹: {scraper_name}")

        scraper_class = self.extensions[scraper_name]
        return scraper_class(**kwargs)
```

---

é€šééµå¾ªæœ¬æ“´å±•æŒ‡å—ï¼Œé–‹ç™¼è€…å¯ä»¥è¼•é¬†åœ°ç‚º SeleniumTCat æ·»åŠ æ–°åŠŸèƒ½ï¼ŒåŒæ™‚ä¿æŒç³»çµ±çš„ä¸€è‡´æ€§å’Œå¯ç¶­è­·æ€§ã€‚